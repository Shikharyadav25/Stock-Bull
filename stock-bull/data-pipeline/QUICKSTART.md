# Stock Bull Data Pipeline - Quick Start Guide

## ðŸš€ Get Started in 30 Minutes (Test Mode)

This guide will help you set up a working data pipeline quickly with a small dataset for testing.

### Step 1: Install Dependencies (5 minutes)
```bash
# Clone or navigate to project
cd stock-bull/data-pipeline

# Create virtual environment
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# Install packages
pip install pandas numpy yfinance nsepy newsapi-python requests beautifulsoup4 sqlalchemy psycopg2-binary python-dotenv schedule transformers torch
```

### Step 2: Setup Database (5 minutes)
```bash
# Install PostgreSQL (if not installed)
# Ubuntu: sudo apt-get install postgresql
# macOS: brew install postgresql
# Windows: Download from postgresql.org

# Create database
createdb stockbull

# Or using psql
psql postgres -c "CREATE DATABASE stockbull;"
```

### Step 3: Configure Environment (2 minutes)

Create `.env` file in `data-pipeline/` directory:
```env
DB_HOST=localhost
DB_PORT=5432
DB_NAME=stockbull
DB_USER=postgres
DB_PASSWORD=your_password

# Optional: Get free key at newsapi.org
NEWS_API_KEY=your_key_here
```

### Step 4: Run Quick Setup (15 minutes)
```bash
# Create tables
python storage/database.py

# Collect data for 5 stocks, last 30 days (quick test)
python << EOF
from datetime import datetime, timedelta
from collectors.stock_price_collector import StockPriceCollector
from collectors.news_collector import NewsCollector
from processors.sentiment_analyzer import SentimentAnalyzer

# Test stocks
stocks = ['RELIANCE', 'TCS', 'INFY', 'HDFCBANK', 'ICICIBANK']
start_date = datetime.now() - timedelta(days=30)

# Collect prices
print("Collecting prices...")
pc = StockPriceCollector()
for stock in stocks:
    pc.collect_stock_prices_yfinance(stock, start_date, datetime.now())

# Collect news
print("Collecting news...")
nc = NewsCollector()
for stock in stocks:
    nc.collect_news_google_rss(stock, max_articles=10)

# Analyze sentiment
print("Analyzing sentiment...")
sa = SentimentAnalyzer()
sa.analyze_news_articles()

print("âœ“ Quick setup complete!")
EOF
```

### Step 5: Verify Data (3 minutes)
```bash
python validators/data_validator.py
# Choose option 3: Generate data quality report
```

You should see:
- âœ… Price data for 5 stocks
- âœ… News articles with sentiment
- âœ… Date range: last 30 days

### Step 6: Generate Features
```bash
python processors/feature_generator.py
# Choose option 3: Generate features for last 30 days
```

## ðŸŽ‰ Success!

You now have:
- Working database with sample data
- Price data + technical indicators
- News data + sentiment analysis
- Features ready for ML training

## Next Steps

### For College Project (Quick):
1. Use the 30-day dataset for model training
2. Train a simple classifier
3. Build basic frontend
4. Demo ready in 1-2 weeks!

### For Full Production:
1. Run `python run_initial_setup.py` for complete data
2. Set up automated scheduler
3. Expand to more stocks
4. Deploy to cloud

## Common Issues

**Database connection error?**
- Check PostgreSQL is running: `sudo service postgresql start`
- Verify `.env` credentials

**No news collected?**
- Google RSS is free and works without API key
- NewsAPI needs free registration

**Import errors?**
- Make sure virtual environment is activated
- Run: `pip install -r requirements.txt`

## Test Your Setup
```python
# Run this to verify everything works
python << EOF
import sys
sys.path.append('.')
from storage.database import DatabaseManager
from sqlalchemy import func
from storage.database import DailyPrice, NewsArticle

db = DatabaseManager()
session = db.get_session()

price_count = session.query(func.count(DailyPrice.id)).scalar()
news_count = session.query(func.count(NewsArticle.id)).scalar()

print(f"âœ“ Price records: {price_count}")
print(f"âœ“ News articles: {news_count}")
print(f"âœ“ Setup successful!" if price_count > 0 else "âœ— No data found")

session.close()
EOF
```

Good luck with your project! ðŸš€
```

## Summary: Complete Data Pipeline Structure
```
stock-bull/
â””â”€â”€ data-pipeline/
    â”œâ”€â”€ config/
    â”‚   â””â”€â”€ config.py                    # Configuration settings
    â”œâ”€â”€ storage/
    â”‚   â””â”€â”€ database.py                   # Database models & connection
    â”œâ”€â”€ collectors/
    â”‚   â”œâ”€â”€ stock_price_collector.py      # Price data collection
    â”‚   â”œâ”€â”€ news_collector.py             # News collection
    â”‚   â””â”€â”€ fundamentals_collector.py     # Fundamental data
    â”œâ”€â”€ processors/
    â”‚   â”œâ”€â”€ technical_indicators.py       # Technical analysis
    â”‚   â”œâ”€â”€ sentiment_analyzer.py         # Sentiment analysis
    â”‚   â””â”€â”€ feature_generator.py          # Feature engineering
    â”œâ”€â”€ validators/
    â”‚   â””â”€â”€ data_validator.py             # Data quality checks
    â”œâ”€â”€ pipeline_scheduler.py             # Automated scheduler
    â”œâ”€â”€ run_initial_setup.py              # One-time setup script
    â”œâ”€â”€ requirements.txt                  # Python dependencies
    â”œâ”€â”€ .env                              # Environment variables
    â”œâ”€â”€ README_DATA_PIPELINE.md           # Full documentation
    â”œâ”€â”€ QUICKSTART.md                     # Quick start guide
    â”œâ”€â”€ raw_data/                         # Temporary storage
    â”œâ”€â”€ processed_data/                   # Generated datasets
    â””â”€â”€ logs/                             # Log files